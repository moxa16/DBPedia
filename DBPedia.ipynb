{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of dbpedia_new.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RoJAfX5C5pk2",
        "K2jKERLcIs1U"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPhmG94yMxiA",
        "colab_type": "text"
      },
      "source": [
        "#  Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSVSLsZhBZjz",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwhHeTWGMnbN",
        "colab_type": "code",
        "outputId": "c1b4403e-1f51-4135-a59c-01c8c32ac89b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmqaldbzM_aK",
        "colab_type": "code",
        "outputId": "d695587c-e096-419f-e6ca-09677f2083b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "!kaggle datasets download -d danofer/dbpedia-classes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/kaggle/api/kaggle_api_extended.py\", line 146, in authenticate\n",
            "    self.config_file, self.config_dir))\n",
            "IOError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAVeO0r_NA16",
        "colab_type": "code",
        "outputId": "e835e917-6c2e-455e-8c13-cb72fd056c5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!unzip db*.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open db*.zip, db*.zip.zip or db*.zip.ZIP.\n",
            "\n",
            "No zipfiles found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLVD_uadNXl8",
        "colab_type": "code",
        "outputId": "551deb12-e14a-4614-e204-88f1d849f4d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-18 09:13:27--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-12-18 09:13:27--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2019-12-18 09:13:27--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.09MB/s    in 6m 27s  \n",
            "\n",
            "2019-12-18 09:19:54 (2.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHNaIAWYNcNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from functools import wraps\n",
        "from time import time\n",
        "\n",
        "def time_wraps(f):\n",
        "  \"\"\"A Decorator function to get the time.\"\"\"\n",
        "  @wraps(f)\n",
        "  def wrapper(*args, **kwargs):\n",
        "    start = time()\n",
        "    result = f(*args, **kwargs)\n",
        "    endtime = time()\n",
        "    print('Elapsed time : {} secs '.format(round((endtime-start), 4)))\n",
        "    return result\n",
        "  return wrapper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC4QD2EFNfQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@time_wraps # - this is a decorator\n",
        "def read_glove(file_name):\n",
        "  \"\"\"A Function to read the glove vectors from file.\"\"\"\n",
        "  with open(file_name,'r') as f:\n",
        "    word_vocab = set() # not using list to avoid duplicate entry\n",
        "    word2vector = {}\n",
        "    for line in f:\n",
        "      line_ = line.strip() #Remove leading and trailing white spaces\n",
        "      words_Vec = line_.split() #splits the string into list\n",
        "      word_vocab.add(words_Vec[0]) #Formation of string\n",
        "      word2vector[words_Vec[0]] = np.array(words_Vec[1:],dtype=float) #formation of array\n",
        "      pdb.set_trace()\n",
        "  print(\"Total Words in DataSet:\",len(word_vocab))\n",
        "  return word_vocab,word2vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x-cjMFpN8dT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17sCfGbFvRB2",
        "colab_type": "code",
        "outputId": "bc313097-4041-49a6-903f-09841f5c3f7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "word_vocab, w2v = read_glove('glove.6B.50d.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-34c458a76659>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_glove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove.6B.50d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-d072f3848b6d>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Elapsed time : {} secs '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-18372cda899c>\u001b[0m in \u001b[0;36mread_glove\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mword_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_Vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Formation of string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mword2vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords_Vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_Vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#formation of array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total Words in DataSet:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mword_vocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword2vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pdb' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRUal88xNhIi",
        "colab_type": "code",
        "outputId": "b4d85bec-80e2-46ea-fef7-5200c5a4b78a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "source": [
        "df = pd.read_csv('DBP_wiki_data.csv')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-24a92b2b0018>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DBP_wiki_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'DBP_wiki_data.csv' does not exist: b'DBP_wiki_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JiysGTIOqVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeZbq3TzKRuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q swifter\n",
        "import swifter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_SZUpjzOCJ1",
        "colab_type": "text"
      },
      "source": [
        "# EDA 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkH3y4q4RmON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@time_wraps\n",
        "def plot_bar(x, y, plt_title):\n",
        "  \"\"\"A Function to plot the bar graph.\"\"\"\n",
        "  fig, ax = plt.subplots(figsize=(35, 7))\n",
        "  ax.bar(x, y)\n",
        "  ax.tick_params('x', rotation=90)\n",
        "  ax.set_title(plt_title)\n",
        "  fig.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3ZU_kXXOAIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# counting the class in dataset\n",
        "l3_value_counts = df.l3.value_counts()\n",
        "l3_value_counts.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1N8hBITZFxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_bar(l3_value_counts.index.to_list(), l3_value_counts.to_list(), 'Plot for class counts')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAKNiPPSQFhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plotting word_count per class\n",
        "plot_bar(df.l3.to_list(), df.word_count.to_list(), 'Plot for word count per class')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoEpVQyTQzD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoXWzb5VRCq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_label_perc(x):\n",
        "  \"\"\"A function to calculate the label percentage in the dataset.\"\"\"\n",
        "  return (l3_value_counts.loc[x] / df.shape[0]) * 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZunPWUfRXO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['perc_label'] = df.l3.apply(cal_label_perc) #added a column of calculated label percentage to df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9bDxdbORe88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYYUZa-GSJqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(1, 4):\n",
        "  new_df = df[df['perc_label'] < (i * 0.25)]\n",
        "  plot_bar(new_df.l3.to_list(), new_df.perc_label.to_list(), 'classes on {} contribution'.format((i * 0.25)))\n",
        "  print('--'.join(list(set(new_df.l3.tolist()))))\n",
        "  del new_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0Rm29_w7KJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmiyRTnH36Zp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.dropna(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxWXFsuo4AfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n12WEkyC4C7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3bvYFdw7K0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, test_data = train_test_split(df, stratify=df.l3, test_size=0.1, shuffle=True, random_state=42)\n",
        "train_data.shape, test_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S-SH8fQL4Sf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_diction = dict((v, i) for i, v in enumerate(list(set(df.l3.to_list())))) #DOUBT\n",
        "rev_class_diction = dict((i, v) for i, v in zip(class_diction.values(), class_diction.keys()))\n",
        "len(class_diction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFdmmoYKMM4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change the class function\n",
        "def class_fn(x):\n",
        "  \"\"\"A Function to convert from the class label to number.\"\"\"\n",
        "  return class_diction[x]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXiQnugIMav5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data.l3 = train_data.l3.apply(class_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dat_XorMyUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW_HThuXTrhy",
        "colab_type": "text"
      },
      "source": [
        "**SVC EXP 1** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E8BZ0ytTq3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import collections"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXkTnzc_U8Zi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TfidfEmbeddingVectorizer(object):\n",
        "  def __init__(self, word2vec):\n",
        "    self.word2vec = word2vec\n",
        "    self.word2weight = None\n",
        "    self.dim = 50\n",
        "  \n",
        "  def fit(self, X, y):\n",
        "    tfidf = TfidfVectorizer(analyzer=lambda x : x)\n",
        "    tfidf.fit(X)\n",
        "    \n",
        "    max_idf = max(tfidf.idf_)\n",
        "    self.word2weight = collections.defaultdict(\n",
        "        lambda: max_idf,\n",
        "        [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()]\n",
        "    )\n",
        "    return self\n",
        "  \n",
        "  def transform(self, X):\n",
        "    tmp = np.array([\n",
        "                     [self.word2vec[w] * self.word2weight[w]\n",
        "                      for w in X if w in self.word2vec] or\n",
        "                     [np.zeros(self.dim)] for words in X\n",
        "    ])\n",
        "    tmp = np.reshape(tmp, (tmp.shape[0], -1))\n",
        "    return tmp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyPo6aInT16O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svc_pipeline = Pipeline([\n",
        "                         ('word2vec_tfidf', TfidfEmbeddingVectorizer(w2v)),\n",
        "                         ('svc_clf', SVC(degree=9, kernel='rbf', gamma='scale', probability=True, random_state=42, verbose=True, class_weight='balanced'))\n",
        "], verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jFukFB3UdLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print('epoch - {}'.format(epoch))\n",
        "  idx = train_data.sample(n=50000, random_state=42).index\n",
        "  svc_pipeline.fit(train_data.text.loc[idx], train_data.l3.loc[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7KfzAHs2gz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13M6etI27Pb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testing the predictions\n",
        "tmp_idx = test_data.sample(n=50, random_state=42).index\n",
        "tmp_pred = svc_pipeline.predict_proba(test_data.text.loc[tmp_idx])\n",
        "tmp_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkCp-E7NyaiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp_pred = [np.argmax(i) for i in tmp_pred]\n",
        "tmp_pred[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMjOffrmNgN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_true = test_data.l3.loc[tmp_idx].apply(class_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE4gz345ORUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classification_report(y_true.to_list(), tmp_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoJAfX5C5pk2",
        "colab_type": "text"
      },
      "source": [
        "# EDA 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQdi1GNc5pU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "import spacy\n",
        "from spacy.en import English()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYUJaMhu78Ne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "STOP_LIST = set(stopwords.words('english') + [\"n't\", \"'s\", \"'m\", \"ca\"] + list(ENGLISH_STOP_WORDS))\n",
        "\n",
        "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"----\",  \"...\", \"“\", \"”\", \"'ve\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwZymPnH8iVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizing using spacy\n",
        "def spacy_tokenize(text):\n",
        "  \"\"\"A Functiont to tokenize the text.\"\"\"\n",
        "  \n",
        "  # break into tokens\n",
        "  tokens = parser(text)\n",
        "  \n",
        "  # lemmatize\n",
        "  lemmas = [tok.lemma_.lower().strip() for tok in tokens if tok.lemma_ != \"-PRON-\" else tok.lower_]\n",
        "  tokens = lemmas\n",
        "  \n",
        "  # remove stop words\n",
        "  tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
        "  \n",
        "  return tokens\n",
        "\n",
        "# function to clean the text\n",
        "def clean_text(text):\n",
        "  \"\"\"A Function to clean the bloody text!!\"\"\"\n",
        "  \n",
        "  # removing spaces and newlines\n",
        "  text = text.strip().replace(\"\\n\", \" the text\n",
        "  spacy_tokens = spacy_tokenize(text)\n",
        "  \n",
        "  # replace chinese symbols\n",
        "  stupid_chinese_symbols = re.findall(ur'[\\u4e00-\\u9fff]+', text)\n",
        "  text = \" \".join([w for w in spacy_tokens if w not in stupid_chinese_symbols])\n",
        "  \n",
        "  # replace html symbols\n",
        "  text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\" )\n",
        "  \n",
        "  # lower the case\n",
        "  text = text.lower()\n",
        "  return text\n",
        "\n",
        "# features with highest coefficient values\n",
        "def print_highest_coeff(vectorizer, clf, N):\n",
        "  pass\n",
        "\n",
        "# custom transformer for cleaning text with spacy\n",
        "class CleanTextTransformer(TransformerMixin):\n",
        "  \"\"\"Clean the bloody text!!\"\"\"\n",
        "  \n",
        "  def transform(self, X, **transform_params):\n",
        "    return [clean_text(text) for text in X]\n",
        "  \n",
        "  def fit(self, X, y=None, **fit_params):\n",
        "    return self\n",
        "  \n",
        "  def get_params(self, deep=True):\n",
        "    return {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV4Z-oBtAF0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a vectorizer\n",
        "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH7hVLqoDzPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the classifier\n",
        "clf = SVC(probability=True, gamma='scale')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPqDKVVsD5dr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a pipeline\n",
        "pipe = Pipeline([\n",
        "    ('clean_text', CleanTextTransformer()),\n",
        "    ('vectorizer', vectorizer),\n",
        "    ('clf', clf)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7042FQXFu0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit the model on train data\n",
        "pipe.fit(train_data.text.to_list(), train_data.l3.to_list())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsnLfUQMF346",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the vectorizer in getting the feature names\n",
        "\n",
        "vocab = vectorizer.get_feature_names()\n",
        "\n",
        "# display the transformed vectors\n",
        "pipe = Pipeline([\n",
        "    ('clean_text', CleantTextTransformer()),\n",
        "    ('vectorizer', vectorizer)\n",
        "])\n",
        "\n",
        "transform = pipe.fit_transform(train_data.text.to_list(), train_data.l3.to_list())\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "  s = \"\"\n",
        "  idx_to_vocab = transform.indices[transform.indptr[i] : transform.indptr[i + 1]]\n",
        "  num_occurences = transform.data[transform.indptr[i] : transform.indptr[i + 1]]\n",
        "  for idx, num in zip(idx_to_vocab, num_occurences):\n",
        "    s += str((vocab[idx], num))\n",
        "  print(\"Sample {}:{}\".format(i, s))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2jKERLcIs1U",
        "colab_type": "text"
      },
      "source": [
        "# EDA 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqE3dn3sGBB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove the sentences with word count less than 5\n",
        "def word_count_based_removal(df, n):\n",
        "  \"\"\"A function to remove the sentences as per the given word count.\"\"\"\n",
        "  return df.loc[df[df.word_count > n].index]\n",
        "\n",
        "# get max min avg of the word count\n",
        "def get_max_min_avg(df):\n",
        "  \"\"\"A Function to get the max min from df.\"\"\"\n",
        "  return max(df.word_count), min(df.word_count), np.average(df.word_count)\n",
        "\n",
        "# Given text count the words\n",
        "def count_words(text):\n",
        "  \"\"\"A function to count the words in the text.\"\"\"\n",
        "  return len(text.split())\n",
        "\n",
        "# remove words as per frequency of the words\n",
        "def freq_based_removal(df, m, n):\n",
        "  \"\"\"A Function to remove as per the frequency.\"\"\"\n",
        "  freq = pd.Series(' '.join(df.text).split()).value_counts()\n",
        "  high_freq = freq[freq > m].index\n",
        "  low_freq = freq[freq < n].index\n",
        "  df.text = df.text.swifter.apply(lambda x: ' '.join(x for x in x.split() if x not in high_freq))\n",
        "  df.text = df.text.swifter.apply(lambda x: ' '.join(x for x in x.split() if x not in low_freq))\n",
        "  df.word_count = df.text.swifter.apply(count_words)\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}